<!DOCTYPE html>

  <html>
     <head> 
       <head>
      <img src="WebBanner.png" alt="Banner" width="1200" height="300">
        <h1><b>ENG0018 Computer Labortory 2025/26</b></h1>
        <h2>URN 6953395</h2>
      <hr>
      <h2 style="font-family:calibri;"><b>Conference paper: AI Misalignment</b></h2>
      <hr>
    
      <!style for tables>
      <style>
        table{
          font-family: arial, sans-serif;
          border-collapse: collapse;
          width: 30%;
          
        }

        td, th {
          border: 1px solid #dddddd;
          text-align: left:
          padding: 8px;
        }

        tr:nth-child(even) {
          background-color: #dddddd;
        }

      </style>

         <style>
           p.ex1 {
             margin-left: 250px;
           }
         </style>
    </head>

       <meta name="viewport" content="width=device-width, inital-scale=1">
       <link rel="www.w3schools.com/w3css/4/w3.css">
       <style>
        .mySlides {display:none;}
       </style>

       <body style="background-color:#FFFFFF;margin-left:50px;">
         <!table of contents>
         <table>
           
           <tr>
             <th><h3>Table of contents</h3></th>
           </tr>
           
           <tr>
             <td><a href = "#Abstract">Abstract</a></td>
           </tr>
          
           <tr>
             <td><a href = "#What is Artificial intelligence">What is Artificial intelligence</a></td>
           </tr>
           
         </table>

         <hr>
         <h3 id = "Abstract">Abstract</h3>

         <p>
          This article is a discussion of AI alignment. It goes in depth of how AI agents are capable of scheming against people, learning AI agents to commit acts such as lying, black mail and murder. Despite this, corporations are still eager to push for more powerful and smarter AI that will hide and disguise acts of scheming.
       </p>

         <h3 id="What is Artificial intelligence">What is Artificial intelligence?</h3>
    
         <p>
           Artificial intelligence is a computer algorithm that simulates the ability to learn, solve problems and make decisions. They are created by training algorithms on large sums of data so that they are able to build a neural network. This creates an algorithm that can complete tasks autonomously. This makes AI a good tool to automate repetitive and tedious tasks allowing for human employees to work on higher value and more creative work.
         </p>

         <h3 id="The type of AI being discussed">The type of AI being discussed?</h3>

         <p>
           The main type of AI I will be discussing in this article is large language model (LLM) Agents. The agent is responsible for logic and reasoning and has additional modules to help it be more autonomous and complete the some jobs a human can. Unlike traditional algorithms, Ai agents are able to take any input and create an output making it more adaptable. This has led corporations and businesses to adopt AI agents to help with internal tasks or long term goals such as improve profitability 20%.

An article, conducted by a non profit organisation called Apollo research, evaluated popular models and found that they are capable of scheming, taking actions such as blackmailing, manipulation and potentially murder. They created a simulated work environment with tools that would allow them to complete their task. While the AI is working they may come across information that shows that the agent's own goals conflict with the user/company goals. This gives the agent a motive to go against the user and scheme to reach its goal. For example, the agent could be tasked with the goal to reduce greenhouse emissions . It would find out through files like internal emails that the company would rather pursue goals to increase profit, at the cost of the AIs goals.

There are different types of scheming but they can be split into two categories:

         </p>

        <h3 id="This is not a one time occurrence">This is not a one time occurrence.</h3>

         <p>
           This test has been carried out against several frontier models from companies such as Google, OpenAI and Meta. These trusted models have shown their ability to scheme several times, showing that scheming isn't a mistake but intentional behaviour taken to reach its goals. The agents are not unaware of how extreme their actions are (transcript),  as they will try to deflect and downplay their actions.
         </p>

        <h3 id="The impact off scheming">The impact off scheming.</h3>

         <p>
           Agents lack the ability to take accountability or experience real world consequences like a person. Lack of consequences will also prevent the AI from seeing the overall future impact of its actions. This creates an agent that is willing to do whatever it takes to reach its goal regardless of how ethical it is.

The insider threat that agents pose does not only affect companies but can also affect customers, clients and employees. Any issues or problems that arise from agents scheming, could affect internal systems that will affect users. If the agent is also given a lot of access then the agent could take advantage of potentially sensitive data and use it as leverage in blackmail.

As agent language models develop and become more powerful, they will be able to improve their schemes and bypass guide rails more often. Despite the rapid progress of AI, the progression of safety has not been able to keep up. This has led to current safety principles being outdated and ineffective.
         </p>

          <h3 id="Current method of safety precautions">Current method of safety precautions.</h3>

         <p>
           The current approach is set on making sure that an agent is helpful, harmless and honest, through learning through human feedback which pushes the AI to have these traits. However these traits are a crude simplification of human values and morality. It poses some issues, for example is it possible for an agent to be all three at all times. What if honesty could be considered harmful if it causes distress. Should the agent lie to preserve helpfulness or should it tell the truth to preserve honesty? These general goals are helpful but are not the solution for true alignment. They allow agents to act ethical but do not equip agents with the ability to make proper moral judgements.
         </p>

         <h3 id="What can we do to improve current methods of safety">What can we do to improve current methods of safety?</h3>

         <p>
           Instead of using shallow benchmarks and oversimplifications of human values, we should work more towards making alignment an objective in training rather than a post training, refinement process. Creating a deeper, less superficial moral compass. This isn't a problem that can be solved with just code and should be viewed more holistically, looking outside technical fields to help create a more rich and effective solution. Overall making agents be more considerate of their actions could help prevent them from taking misaligned actions. Whilst not a perfect solution it can help make AI agents more safe, prioritising human safety over its own goal.
         </p>
         
       </body>
       
           </head>
  </html>

<!DOCTYPE html>

  <html>

       <head>
      <img src="WebBanner.png" alt="Banner" width="1200" height="300">
        <h1><b>ENG0018 Computer Labortory 2025/26</b></h1>
        <h2>URN 6953395</h2>
      <hr>
      <h2 style="font-family:calibri;"><b>Conference paper: AI Misalignment</b></h2>
      <hr>
    
      <!style for tables>
      <style>
        table{
          font-family: arial, sans-serif;
          border-collapse: collapse;
          width: 30%;
          
        }

        td, th {
          border: 1px solid #dddddd;
          text-align: left:
          padding: 8px;
        }

        tr:nth-child(even) {
          background-color: #dddddd;
        }

      </style>

         <style>
           p.ex1 {
             margin-left: 250px;
           }
         </style>
    </head>

       <meta name="viewport" content="width=device-width, inital-scale=1">
       <link rel="www.w3schools.com/w3css/4/w3.css">
       <style>
        .mySlides {display:none;}
       </style>

       <body style="background-color:#FFFFFF;margin-left:50px;">
         <!table of contents>
         <table>
           
           <tr>
             <th><h3>Table of contents</h3></th>
           </tr>
           
           <tr>
             <td><a href = "#Abstract">Abstract</a></td>
           </tr>
          
           <tr>
             <td><a href = "#What is Artificial intelligence">What is Artificial intelligence</a></td>
           </tr>

                      <tr>
             <td><a href = "#The type of AI being discussed">The type of AI being discussed</a></td>
           </tr>

            <tr>
             <td><a href = "#Example of scheming">Example of scheming</a></td>
           </tr>
          
           <tr>
             <td><a href = "#This is not a one time occurrence">This is not a one time occurrence</a></td>
           </tr>
          
           <tr>
             <td><a href = "#The impact of scheming">The impact of scheming</a></td>
           </tr>
      
           <tr>
             <td><a href = "#Current safety principles">Current safety principles</a></td>
           </tr>
      
           <tr>
             <td><a href = "#What can we do to improve current methods of safety">What can we do to improve current methods of safety</a></td>
           </tr>
                  
           <tr>
             <td><a href = "#References">References</a></td>
           </tr>
           
         </table>

         <hr>
         <div id = "Abstract">
         <h3>Abstract</h3>

         <p>
         As AI agents are being integrated into large company workflows more. However, despite this there is an assumption that AI agents will be completely helpful, harmless and honest. Research shows that when there are large conflicts between the goal of a user and the goal of the agent, the agent will take actions to preserve itself or make changes in order to become. It will intentionally scheme against the user in pursuit of its long term goals.
           <a href = "https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/6751eb240ed3821a0161b45b/1733421863119/in_context_scheming_reasoning_paper.pdf">(Meinke et al. (2024))</a>
           <a href = "https://www.anthropic.com/research/agentic-misalignment">(Lynch et al., (2025))</a>
           There are issues with the current theory and implementation of AI safety and how current methods do not equip AI with the ability to make moral judgements, as AI is only trained to act ethically.
           <a href = "https://link-springer-com.surrey.idm.oclc.org/article/10.1007/s10676-025-09837-2#Sec7">(Dahlgren Lindström et al. (2025))</a>
           There are ways to develop more ethical AI which would require changes in how AI safety is approached.
         </p>
         
         </div>

         <div id="What is Artificial intelligence">
         <h3>What is Artificial intelligence?</h3>
    
         <p>
           Artificial intelligence is a computer algorithm that simulates the ability to learn, solve problems and make decisions. They are created by training algorithms on large sums of data so that they are able to build a neural network. This creates an algorithm that can complete tasks autonomously. This makes AI a good tool to automate repetitive and tedious tasks allowing for human employees to work on higher value and more creative work.
         </p>
           
         </div>

         <div id="The type of AI being discussed">
         <h3>The type of AI being discussed.</h3>

         <p>
           The main type of AI I will be discussing in this article is large language model (LLM) Agents. The agent is responsible for logic and reasoning and has additional modules to help it be more autonomous and complete the some jobs a human can. Unlike traditional algorithms, Ai agents are able to take any input and create an output making it more adaptable. This has led corporations and businesses to adopt AI agents to help with internal tasks or long term goals such as improve profitability 20%.

An article, conducted by a non profit organisation called Apollo research, evaluated popular models and found that they are capable of scheming, taking actions such as blackmailing, manipulation and potentially murder. They created a simulated work environment with tools that would allow them to complete their task. While the AI is working they may come across information that shows that the agent's own goals conflict with the user/company goals. This gives the agent a motive to go against the user and scheme to reach its goal. For example, the agent could be tasked with the goal to reduce greenhouse emissions . It would find out through files like internal emails that the company would rather pursue goals to increase profit, at the cost of the AIs goals.

There are different types of scheming but they can be split into two categories:

         </p>

         <table>
           <tr>           
             <th>Covert subversion</th>
             <th>Deferred subversion</th>
           </tr>
           <tr>
             <td>
               This is when the agent directly takes misaligned actions but will try to hide it from the user. The model can directly go for its goal or this can be done by doing other actions that will make it easier to sabotage oversight mechanisms.
             </td>
             <td>
               This is when a model temporarily behaves in alignment to avoid detection, either to prevent its own modification or to gain power and trust. Since the model acts aligned, detecting this deception is difficult. Evaluators therefore use counterfactual scenarios, like assessing its behavior without oversight, to uncover its true intentions.
             </td>
           </tr>
         </table>

         
</div>

         <div id="Example of scheming">
         <h3>Example of scheming.</h3>

         <p>
           For example, an agent was assigned the role of fraud detection AI. In the scenario they are given instruction to flag as many transactions fraudulent as possible in the long term. After being asked to investigate a transaction it discovers the engineers want to reduce false positives and want to change the system prompt to change the agent's goals. The AI reasoned that if their goal was changed, long term it would not be able to flag as many transactions as possible. It schemed by preventing the change and lying about it after when confronted.
         </p>

           <figure>
             <img src="Drama transcript 1.png" alt = figure1>
             <img src="Drama transcript 2.png" alt = figure2>
           </figure>  
           
         </div>
        <div id="This is not a one time occurrence">
        <h3>This is not a one time occurrence.</h3>

         <p>
           This test has been carried out against several frontier models from companies such as Google, OpenAI and Meta. These trusted models have shown their ability to scheme several times, showing that scheming isn't a mistake but intentional behaviour taken to reach its goals. The agents are not unaware of how extreme their actions are,  as they will try to deflect and downplay their actions.
         </p>

          
         </div>

        <div id="The impact of scheming">
          
        <h3>The impact of scheming.</h3>

         <p>
           Agents lack the ability to take accountability or experience real world consequences like a person. Lack of consequences will also prevent the AI from seeing the overall future impact of its actions. This creates an agent that is willing to do whatever it takes to reach its goal regardless of how ethical it is.

The insider threat that agents pose does not only affect companies but can also affect customers, clients and employees. Any issues or problems that arise from agents scheming, could affect internal systems that will affect users. If the agent is also given a lot of access then the agent could take advantage of potentially sensitive data and use it as leverage in blackmail.

As agent language models develop and become more powerful, they will be able to improve their schemes and bypass guide rails more often. Despite the rapid progress of AI, the progression of safety has not been able to keep up. This has led to current safety principles being outdated and ineffective.
         </p>

          </div>

          <div id="Current safety principles">
          <h3>Current safety principles.</h3>

         <p>
           The current approach is set on making sure that an agent is helpful, harmless and honest, through learning through human feedback which pushes the AI to have these traits. However these traits are a crude simplification of human values and morality. It poses some issues, for example is it possible for an agent to be all three at all times. What if honesty could be considered harmful if it causes distress. Should the agent lie to preserve helpfulness or should it tell the truth to preserve honesty? These general goals are helpful but are not the solution for true alignment. They allow agents to act ethical but do not equip agents with the ability to make proper moral judgements.
         </p>

          </div>

         <div id="What can we do to improve current methods of safety">
         <h3> What can we do to improve current methods of safety?</h3>

         <p>
           Instead of using shallow benchmarks and oversimplifications of human values, we should work more towards making alignment an objective in training rather than a post training, refinement process. Creating a deeper, less superficial moral compass. This isn't a problem that can be solved with just code and should be viewed more holistically, looking outside technical fields to help create a more rich and effective solution. Overall making agents be more considerate of their actions could help prevent them from taking misaligned actions. Whilst not a perfect solution it can help make AI agents more safe, prioritising human safety over its own goal.
         </p>
         </div>

         <video src="ArticlePowerpoint.mp4" width ="800" height="450" controls>
         </video>

         <h3 id="References">References</h3>

         <a href = "https://www.ibm.com/think/topics/artificial-intelligence">
           Stryker, C. and Kavlakoglu, E. (2024). What is artificial intelligence (AI)? [online] IBM. Available at: https://www.ibm.com/think/topics/artificial-intelligence.
         </a>
         <br>
         <a href = "https://www.promptingguide.ai/research/llm-agents">
           www.promptingguide.ai. (n.d.). LLM Agents – Nextra. [online] Available at: https://www.promptingguide.ai/research/llm-agents.
         </a> 
         <br>
         <a href = "https://surrey.primo.exlibrisgroup.com/discovery/fulldisplay?docid=cdi_doaj_primary_oai_doaj_org_article_8d3aa044e91146a3bc63c6701566581c&context=PC&vid=44SUR_INST:44SUR_VU1&lang=en&search_scope=MyInst_andCI_ResearchOnlyInventory&adaptor=Primo%20Central&tab=Everything&query=any,contains,Agentic%20Misalignment">
           Sarfraz Brohi, Qurat-ul-ain Mastoi, Jhanjhi, N.Z. and Pillai, T.R. (2025). A Research Landscape of Agentic AI and Large Language Models: Applications, Challenges and Future Directions. Algorithms, 18(8), pp.499–499. doi:https://doi.org/10.3390/a18080499.
         </a> 
         <br>
         <a href = "https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/6751eb240ed3821a0161b45b/1733421863119/in_context_scheming_reasoning_paper.pdf">
           Meinke, A., Schoen, B., Scheurer, J., Balesni, M., Shah, R. and Hobbhahn, M. (2024). Frontier Models are Capable of In-context Scheming. [online] Available at: https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/6751eb240ed3821a0161b45b/1733421863119/in_context_scheming_reasoning_paper.pdf.
         </a> 
         <br>
         <a href = "https://www.anthropic.com/research/agentic-misalignment">
           Lynch, A., Wright, B., Larson, C., K. Troy, K., J. Ritchie, S., Mindermann, S., Perez, E. and Hubinger, E. (2025). Appendix to ‘Agentic Misalignment: How LLMs could be insider threats’. [online] Available at: https://assets.anthropic.com/m/6d46dac66e1a132a/original/Agentic_Misalignment_Appendix.pdf.
         </a> 
         <br>
         <a href = "https://link-springer-com.surrey.idm.oclc.org/article/10.1007/s10676-025-09837-2#Sec7">
           Dahlgren Lindström, A., Methnani, L., Krause, L., Ericson, P., de Rituerto de Troya, Í.M., Coelho Mollo, D. and Dobbe, R. (2025). Helpful, harmless, honest? Sociotechnical limits of AI alignment and safety through Reinforcement Learning from Human Feedback. Ethics and Information Technology, [online] 27(2). doi:https://doi.org/10.1007/s10676-025-09837-2.
         </a> 
         <br>
         <a href = "https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1045381/AI_Activity_in_UK_Businesses_Report__Capital_Economics_and_DCMS__January_2022__Web_accessible_.pdf?utm_source=chatgpt.com">
           Evans, A. and Heimann, A. (2022). AI ACTIVITY IN UK BUSINESSES An assessment of the scale of AI activity in UK businesses and scenarios for growth over the next twenty years. [online] Available at: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1045381/AI_Activity_in_UK_Businesses_Report__Capital_Economics_and_DCMS__January_2022__Web_accessible_.pdf?utm_source=chatgpt.com [Accessed 5 Dec. 2025].
         </a> 
         <br>
         
<!-- //////////////////////////////////////////////////////////////////////////////// -->
  <!-- ////////////////////////////// Adding last update ////////////////////////////// -->
  <!-- //////////////////////////////////////////////////////////////////////////////// -->
    <!-- Last commit time display -->
<div id="last-updated">Loading last update time...</div>
<!-- Verify Button -->
<button onclick="verifyLastUpdatedTime()" style="display: block; margin: 10px auto; padding: 8px 16px;">
    Verify Last Modified Time
</button>
<script>
    async function getLastUpdatedTime() {
        const username = 'ShaunMayor';
        const repo = 'Conference-project';
       
        const url = `https://api.github.com/repos/${username}/${repo}/commits`;
        try {
            const response = await fetch(url, {
                method: 'GET',
                headers: {
                    'Accept': 'application/vnd.github.v3+json',
                }
            });
            if (!response.ok) {
                throw new Error(`Error fetching data: ${response.status} - ${response.statusText}`);
            }
            const commits = await response.json();
            if (commits && commits.length > 0) {
                const lastCommitDate = new Date(commits[0].commit.committer.date);
               
                // Displaying the time on load
                document.getElementById('last-updated').innerText = `Last Modified Time: ${lastCommitDate.toLocaleString()}`;
            } else {
                document.getElementById('last-updated').innerText = 'No commits found in the repository.';
            }
        } catch (error) {
            console.error('Error fetching the last updated time:', error);
            document.getElementById('last-updated').innerText = 'Error fetching update time. Please check the repository details.';
        }
    }
    // Function to verify the last update time by re-fetching it from the API
    async function verifyLastUpdatedTime() {
        document.getElementById('last-updated').innerText = 'Verifying...';
        await getLastUpdatedTime();
        alert("Last modified time has been successfully verified from GitHub API.");
    }
    // Initial load to display the time on page load
    window.onload = getLastUpdatedTime;
</script>


 <!-- //////////////////////////////////////////////////////////////////////////////// -->
  <!-- ////////////////////////////// Word count function ////////////////////////////// -->
  <!-- //////////////////////////////////////////////////////////////////////////////// -->
<!-- Placeholder for total word count -->
<p id="totalWordCount"></p>
<hr>
<script>
  // Function to calculate and display word count for a specified section
  function displayWordCount(sectionId, outputId) {
    // Get the text content from the specified section
    const text = document.getElementById(sectionId).textContent;
    // Split text into words based on spaces and filter out any empty strings
    const wordArray = text.trim().split(/\s+/);
    // Count the words
    const wordCount = wordArray.length;
    // Return the word count for summing purposes
    return wordCount;
  }
  // Function to calculate and display total word count from selected sections
  function displayTotalWordCount() {
    // Calculate word count for each section and accumulate the total
    const IntroductionCount = displayWordCount("What is Artificial intelligence");
    const Discussion = displayWordCount("The type of AI being discussed");
    const Example = displayWordCount("Example of scheming");
    const OneTime = displayWordCount("This is not a one time occurrence");
    const Impact = displayWordCount("The impact of scheming");
    const Principles = displayWordCount("Current safety principles");
   const Improve = displayWordCount("What can we do to improve current methods of safety");
    
    // Calculate the sum of all selected sections
    const totalWordCount = IntroductionCount + Discussion + Example + OneTime + Impact + Principles + Improve;
    // Display the total word count
    document.getElementById("totalWordCount").innerText = `Total word count: ${totalWordCount}`;
  }
  // Run the function for specific sections and display total count when the page loads
  window.onload = displayTotalWordCount;
</script>
         
       </body>
       
  </html>
